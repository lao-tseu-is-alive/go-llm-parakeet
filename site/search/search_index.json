{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Home","text":""},{"location":"#parakeet","title":"\ud83e\udd9c\ud83e\udeba Parakeet","text":"<p>Parakeet is the simplest Go library to create GenAI apps with Ollama.</p> <p>A GenAI app is an application that uses generative AI technology. Generative AI can create new text, images, or other content based on what it's been trained on. So a GenAI app could help you write a poem, design a logo, or even compose a song! These are still under development, but they have the potential to be creative tools for many purposes. - Gemini</p> <p>\u270b Parakeet is only for creating GenAI apps generating text (not image, music,...).</p>"},{"location":"#install","title":"Install","text":"<p>Note</p> <p>current release: <code>v0.2.0 \ud83c\udf55 [pizza]</code></p> <pre><code>go get github.com/parakeet-nest/parakeet\n</code></pre>"},{"location":"#getting-started-first-completion","title":"\ud83d\ude80 Getting Started - First completion","text":"<p><code>generate</code></p> <p>The simple completion can be used to generate a response for a given prompt with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n    })\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, question)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"chat-completion/","title":"Chat completion","text":""},{"location":"chat-completion/#chat-completion","title":"Chat completion","text":""},{"location":"chat-completion/#completion","title":"Completion","text":"<p>The chat completion can be used to generate a conversational response for a given set of messages with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"deepseek-coder\"\n\n    systemContent := `You are an expert in computer programming.\n    Please make friendly answer for the noobs.\n    Add source code examples if you can.`\n\n    userContent := `I need a clear explanation regarding the following question:\n    Can you create a \"hello world\" program in Golang?\n    And, please, be structured with bullet points`\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n        option.RepeatLastN: 2,\n        option.RepeatPenalty: 2.0,\n    })\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n        Stream: false,\n    }\n\n    answer, err := completion.Chat(ollamaUrl, query)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Message.Content)\n}\n</code></pre> <p>\u270b To keep a conversational memory for the next chat completion, update the list of messages with the previous question and answer.</p>"},{"location":"chat-completion/#completion-with-stream","title":"Completion with stream","text":"<pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"deepseek-coder\"\n\n    systemContent := `You are an expert in computer programming.\n    Please make friendly answer for the noobs.\n    Add source code examples if you can.`\n\n    userContent := `I need a clear explanation regarding the following question:\n    Can you create a \"hello world\" program in Golang?\n    And, please, be structured with bullet points`\n\n    options := llm.Options{\n        Temperature: 0.5,\n        RepeatLastN: 2, \n    }\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n        Stream:  false,\n    }\n\n    _, err := completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n}\n</code></pre>"},{"location":"chat-completion/#chat-completion-with-conversational-memory","title":"Chat completion with conversational memory","text":""},{"location":"chat-completion/#in-memory-history","title":"In memory history","text":"<p>To store the messages in memory, use <code>history.MemoryMessages</code></p> <pre><code>package main\n\nimport (\n    \"fmt\"\n    \"log\"\n\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/history\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\" // fast, and perfect answer (short, brief)\n\n    conversation := history.MemoryMessages{\n        Messages: make(map[string]llm.MessageRecord),\n    }\n\n    systemContent := `You are an expert with the Star Trek series. use the history of the conversation to answer the question`\n\n    userContent := `Who is James T Kirk?`\n\n    options := llm.Options{\n        Temperature: 0.5,\n        RepeatLastN: 2,  \n    }\n\n    query := llm.Query{\n        Model: model,\n        Messages: []llm.Message{\n            {Role: \"system\", Content: systemContent},\n            {Role: \"user\", Content: userContent},\n        },\n        Options: options,\n    }\n\n    // Ask the question\n    answer, err := completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        },\n    )\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    // Save the conversation\n    _, err = conversation.SaveMessage(\"1\", llm.Message{\n        Role:    \"user\",\n        Content: userContent,\n    })\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    _, err = conversation.SaveMessage(\"2\", llm.Message{\n        Role:    \"system\",\n        Content: answer.Message.Content,\n    })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n    // New question\n    userContent = `Who is his best friend ?`\n\n    previousMessages, _ := conversation.GetAllMessages()\n\n    // (Re)Create the conversation\n    conversationMessages := []llm.Message{}\n    // instruction\n    conversationMessages = append(conversationMessages, llm.Message{Role: \"system\", Content: systemContent})\n    // history\n    conversationMessages = append(conversationMessages, previousMessages...)\n    // last question\n    conversationMessages = append(conversationMessages, llm.Message{Role: \"user\", Content: userContent})\n\n    query = llm.Query{\n        Model:    model,\n        Messages: conversationMessages,\n        Options:  options,\n    }\n\n    answer, err = completion.ChatStream(ollamaUrl, query,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Message.Content)\n            return nil\n        },\n    )\n    fmt.Println()\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n\n}\n</code></pre>"},{"location":"chat-completion/#bbolt-history","title":"Bbolt history","text":"<p>Bbolt is an embedded key/value database for Go.</p> <p>To store the messages in a bbolt bucket, use <code>history.BboltMessages</code></p> <pre><code>conversation := history.BboltMessages{}\nconversation.Initialize(\"../conversation.db\")\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/11-chat-conversational-bbolt</li> <li>examples/11-chat-conversational-bbolt/begin: start a conversation and save the history</li> <li>examples/11-chat-conversational-bbolt/resume: load the messages from the history bucket and resue the conversation</li> </ul>"},{"location":"chunkers-and-splitters/","title":"Chunkers and Splitters","text":"<p>There are several methods in the <code>content</code> package to help you chunk and split text:</p> <ul> <li><code>ChunkText</code> takes a text string and divides it into chunks of a specified size with a given overlap. It returns a slice of strings, where each string represents a chunk of the original text.</li> </ul> <pre><code>chunks := content.ChunkText(documentContent, 900, 400)\n</code></pre> <ul> <li><code>SplitTextWithDelimiter</code> splits the given text using the specified delimiter and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitTextWithDelimiter(documentContent, \"&lt;!-- SPLIT --&gt;\")\n</code></pre> <ul> <li><code>SplitTextWithRegex</code> splits the given text using the provided regular expression delimiter. It returns a slice of strings containing the split parts of the text.</li> </ul> <pre><code>chunks := content.SplitTextWithRegex(documentContent, `## *`)\n</code></pre> <ul> <li><code>SplitMarkdownBySections</code> splits the given markdown text using the title sections (<code>#, ##, etc.</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitMarkdownBySections(documentContent)\n</code></pre> <ul> <li><code>SplitAsciiDocBySections</code> splits the given asciidoc text using the title sections (<code>=, ==, etc.</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitAsciiDocBySections(documentContent)\n</code></pre> <ul> <li><code>SplitHTMLBySections</code> splits the given html text using the title sections (<code>h1, h2, h3, h4, h5, h6</code>) and returns a slice of strings.</li> </ul> <pre><code>chunks := content.SplitHTMLBySections(documentContent)\n</code></pre>"},{"location":"embeddings/","title":"Embeddings","text":""},{"location":"embeddings/#embeddings","title":"Embeddings","text":""},{"location":"embeddings/#create-embeddings","title":"Create embeddings","text":"<pre><code>embedding, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  \"all-minilm\",\n        Prompt: \"Jean-Luc Picard is a fictional character in the Star Trek franchise.\",\n    },\n    \"Picard\", // identifier\n)\n</code></pre>"},{"location":"embeddings/#vector-stores","title":"Vector stores","text":"<p>A vector store allows to store and search for embeddings in an efficient way.</p>"},{"location":"embeddings/#in-memory-vector-store","title":"In memory vector store","text":"<p>Create a store: <pre><code>store := embeddings.MemoryVectorStore{\n    Records: make(map[string]llm.VectorRecord),\n}\n</code></pre></p> <p>Save embeddings: <pre><code>store.Save(embedding)\n</code></pre></p> <p>Search embeddings: <pre><code>embeddingFromQuestion, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  \"all-minilm\",\n        Prompt: \"Who is Jean-Luc Picard?\",\n    },\n    \"question\",\n)\n// find the nearest vector\nsimilarity, _ := store.SearchMaxSimilarity(embeddingFromQuestion)\n\ndocumentsContent := `&lt;context&gt;&lt;doc&gt;` + similarity.Prompt + `&lt;/doc&gt;&lt;/context&gt;`\n</code></pre></p> <p>\ud83d\udc40 you will find a complete example in <code>examples/08-embeddings</code></p>"},{"location":"embeddings/#bbolt-vector-store","title":"Bbolt vector store","text":"<p>Bbolt is an embedded key/value database for Go.</p> <p>Create a store, and open an existing store: <pre><code>store := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/09-embeddings-bbolt</li> <li>examples/09-embeddings-bbolt/create-embeddings: create and populate the vector store</li> <li>examples/09-embeddings-bbolt/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#redis-vector-store","title":"Redis vector store","text":"<p>Create a store, and open an existing store: <pre><code>redisStore := embeddings.RedisVectorStore{}\nerr := redisStore.Initialize(\"localhost:6379\", \"\", \"chronicles-bucket\")\n\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/32-rag-with-redis</li> <li>examples/32-rag-with-redis/create-embeddings: create and populate the vector store</li> <li>examples/32-rag-with-redis/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#elasticsearch-vector-store","title":"Elasticsearch vector store","text":"<p>Create a store, and open an existing store: <pre><code>cert, _ := os.ReadFile(os.Getenv(\"ELASTIC_CERT_PATH\"))\n\nelasticStore := embeddings.ElasticSearchStore{}\nerr := elasticStore.Initialize(\n    []string{\n        os.Getenv(\"ELASTIC_ADDRESS\"),\n    },\n    os.Getenv(\"ELASTIC_USERNAME\"),\n    os.Getenv(\"ELASTIC_PASSWORD\"),\n    cert,\n    \"chronicles-index\",\n)\n</code></pre></p> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/33-rag-with-elastic</li> <li>examples/33-rag-with-elastic/create-embeddings: create and populate the vector store</li> <li>examples/33-rag-with-elastic/use-embeddings: search similarities in the vector store</li> </ul>"},{"location":"embeddings/#additional-data","title":"Additional data","text":"<p>you can add additional data to a vector record (embedding):</p> <pre><code>embedding.Text()\nembedding.Reference()\nembedding.MetaData()\n</code></pre>"},{"location":"embeddings/#create-embeddings-from-text-files-and-similarity-search","title":"Create embeddings from text files and Similarity search","text":""},{"location":"embeddings/#create-embeddings_1","title":"Create embeddings","text":"<pre><code>ollamaUrl := \"http://localhost:11434\"\nembeddingsModel := \"all-minilm\"\n\nstore := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n\n// Parse all golang source code of the examples\n// Create embeddings from documents and save them in the store\ncounter := 0\n_, err := content.ForEachFile(\"../../examples\", \".go\", func(path string) error {\n    data, err := os.ReadFile(path)\n    if err != nil {\n        return err\n    }\n\n    fmt.Println(\"\ud83d\udcdd Creating embedding from:\", path)\n    counter++\n    embedding, err := embeddings.CreateEmbedding(\n        ollamaUrl,\n        llm.Query4Embedding{\n            Model:  embeddingsModel,\n            Prompt: string(data),\n        },\n        strconv.Itoa(counter), // don't forget the id (unique identifier)\n    )\n    fmt.Println(\"\ud83d\udce6 Created: \", len(embedding.Embedding))\n\n    if err != nil {\n        fmt.Println(\"\ud83d\ude21:\", err)\n    } else {\n        _, err := store.Save(embedding)\n        if err != nil {\n            fmt.Println(\"\ud83d\ude21:\", err)\n        }\n    }\n    return nil\n})\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\n</code></pre>"},{"location":"embeddings/#similarity-search","title":"Similarity search","text":"<pre><code>ollamaUrl := \"http://localhost:11434\"\nembeddingsModel := \"all-minilm\"\nchatModel := \"magicoder:latest\"\n\nstore := embeddings.BboltVectorStore{}\nstore.Initialize(\"../embeddings.db\")\n\nsystemContent := `You are a Golang developer and an expert in computer programming.\nPlease make friendly answer for the noobs. Use the provided context and doc to answer.\nAdd source code examples if you can.`\n\n// Question for the Chat system\nuserContent := `How to create a stream chat completion with Parakeet?`\n\n// Create an embedding from the user question\nembeddingFromQuestion, err := embeddings.CreateEmbedding(\n    ollamaUrl,\n    llm.Query4Embedding{\n        Model:  embeddingsModel,\n        Prompt: userContent,\n    },\n    \"question\",\n)\nif err != nil {\n    log.Fatalln(\"\ud83d\ude21:\", err)\n}\nfmt.Println(\"\ud83d\udd0e searching for similarity...\")\n\nsimilarities, _ := store.SearchSimilarities(embeddingFromQuestion, 0.3)\n\n// Generate the context from the similarities\n// This will generate a string with a content like this one:\n// `&lt;context&gt;&lt;doc&gt;...&lt;doc&gt;&lt;doc&gt;...&lt;doc&gt;&lt;/context&gt;`\ndocumentsContent := embeddings.GenerateContextFromSimilarities(similarities)\n\nfmt.Println(\"\ud83c\udf89 similarities\", len(similarities))\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.4,\n    option.RepeatLastN: 2,\n})\n\nquery := llm.Query{\n    Model: chatModel,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"system\", Content: documentsContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n    Stream: false,\n}\n\nfmt.Println(\"\")\nfmt.Println(\"\ud83e\udd16 answer:\")\n\n// Answer the question\n_, err = completion.ChatStream(ollamaUrl, query,\n    func(answer llm.Answer) error {\n        fmt.Print(answer.Message.Content)\n        return nil\n    })\n\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre>"},{"location":"embeddings/#other-similarity-search-methods","title":"Other similarity search methods","text":"<p><code>SearchMaxSimilarity</code> searches for the vector record in the <code>BboltVectorStore</code> that has the maximum cosine distance similarity to the given <code>embeddingFromQuestion</code>: <pre><code>similarity, _ := store.SearchMaxSimilarity(embeddingFromQuestion)\n</code></pre></p> <p><code>SearchTopNSimilarities</code> searches for vector records in the <code>BboltVectorStore</code> that have a cosine distance similarity greater than or equal to the given <code>limit</code> and returns the top <code>n</code> records: <pre><code>similarities, _ := store.SearchTopNSimilarities(embeddingFromQuestion, limit, n)\n</code></pre></p>"},{"location":"function-calling-before-tools-support/","title":"Function Calling (before tools support)","text":""},{"location":"function-calling-before-tools-support/#function-calling-before-tool-support","title":"Function Calling (before tool support)","text":"<p>almost depecrated</p> <p>What is \"Function Calling\"? First, it's not a feature where a LLM can call and execute a function. \"Function Calling\" is the ability for certain LLMs to provide a specific output with the same format (we could say: \"a predictable output format\").</p> <p>So, the principle is simple:</p> <ul> <li>You (or your GenAI application) will create a prompt with a delimited list of tools (the functions) composed by name, descriptions, and parameters: <code>SayHello</code>, <code>AddNumbers</code>, etc.</li> <li>Then, you will add your question (\"Hey, say 'hello' to Bob!\") to the prompt and send all of this to the LLM.</li> <li>If the LLM \"understand\" that the <code>SayHello</code> function can be used to say \"hello\" to Bob, then the LLM will answer with only the name of the function with the parameter(s). For example: <code>{\"name\":\"SayHello\",\"arguments\":{\"name\":\"Bob\"}}</code>.</li> </ul> <p>Then, it will be up to you to implement the call of the function.</p> <p>The latest version (v0.3) of Mistral 7b supports function calling and is available for Ollama.</p>"},{"location":"function-calling-before-tools-support/#define-a-list-of-tools","title":"Define a list of tools","text":"<p>First, you have to provide the LLM with a list of tools with the following format:</p> <pre><code>toolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with his name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"function-calling-before-tools-support/#generate-a-prompt-from-the-tools-list-and-the-user-instructions","title":"Generate a prompt from the tools list and the user instructions","text":"<p>The <code>tools.GenerateContent</code> method generates a string with the tools in JSON format surrounded by <code>[AVAILABLE_TOOLS]</code> and <code>[/AVAILABLE_TOOLS]</code>: <pre><code>toolsContent, err := tools.GenerateContent(toolsList)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre></p> <p>The <code>tools.GenerateInstructions</code> method generates a string with the user instructions surrounded by <code>[INST]</code> and <code>[/INST]</code>: <pre><code>userContent := tools.GenerateInstructions(`say \"hello\" to Bob`)\n</code></pre></p> <p>Then, you can add these two strings to the messages list: <pre><code>messages := []llm.Message{\n    {Role: \"system\", Content: toolsContent},\n    {Role: \"user\", Content: userContent},\n}\n</code></pre></p>"},{"location":"function-calling-before-tools-support/#send-the-prompt-messages-to-the-llm","title":"Send the prompt (messages) to the LLM","text":"<p>It's important to set the <code>Temperature</code> to <code>0.0</code>: <pre><code>options := llm.Options{\n    Temperature:   0.0,\n    RepeatLastN:   2,\n    RepeatPenalty: 2.0,\n}\n\n//You must set the `Format` to `json` and `Raw` to `true`:\nquery := llm.Query{\n    Model: model,\n    Messages: messages,\n    Options: options,\n    Format:  \"json\",\n    Raw:     true,\n}\n</code></pre></p> <p>When building the payload to be sent to Ollama, we need to set the <code>Raw</code> field to true, thanks to that, no formatting will be applied to the prompt (we override the prompt template of Mistral), and we need to set the <code>Format</code> field to <code>\"json\"</code>.</p> <p>No you can call the <code>Chat</code> method. The answer of the LLM will be in JSON format: <pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n// PrettyString is a helper that prettyfies the JSON string\nresult, err := gear.PrettyString(answer.Message.Content)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre></p> <p>You should get this answer: <pre><code>{\n  \"name\": \"hello\",\n  \"arguments\": {\n    \"name\": \"Bob\"\n  }\n}\n</code></pre></p> <p>You can try with the other tool (or function): <pre><code>userContent := tools.GenerateInstructions(`add 2 and 40`)\n</code></pre></p> <p>You should get this answer: <pre><code>{\n  \"name\": \"addNumbers\",\n  \"arguments\": {\n    \"a\": 2,\n    \"b\": 40\n  }\n}\n</code></pre></p> <p>Remark: always test the format of the output, even if Mistral is trained for \"function calling\", the result are not entirely predictable.</p> <p>Note</p> <p>Look at this sample for a complete sample: examples/15-mistral-function-calling</p>"},{"location":"function-calling-without-tools-support/","title":"Function Calling (without tools support)","text":""},{"location":"function-calling-without-tools-support/#function-calling-with-llms-that-do-not-implement-tools-support","title":"Function Calling with LLMs that do not implement tools support","text":"<p>It is possible to reproduce this feature with some LLMs that do not implement the \"Function Calling\" feature natively, but we need to supervise them and explain precisely what we need. The result (the output) will be less predictable, so you will need to add some tests before using the output, but with some \"clever\" LLMs, you will obtain correct results. I did my experiments with phi3:mini.</p> <p>The trick is simple:</p> <p>Add this message at the begining of the list of messages: <pre><code>systemContentIntroduction := `You have access to the following tools:`\n</code></pre></p> <p>Add this message at the end of the list of messages, just before the user message: <pre><code>systemContentInstructions := `If the question of the user matched the description of a tool, the tool will be called.\nTo call a tool, respond with a JSON object with the following structure: \n{\n    \"name\": &lt;name of the called tool&gt;,\n    \"arguments\": {\n    &lt;name of the argument&gt;: &lt;value of the argument&gt;\n    }\n}\n\nsearch the name of the tool in the list of tools with the Name field\n`\n</code></pre></p> <p>At the end, you will have this: <pre><code>messages := []llm.Message{\n    {Role: \"system\", Content: systemContentIntroduction},\n    {Role: \"system\", Content: toolsContent},\n    {Role: \"system\", Content: systemContentInstructions},\n    {Role: \"user\", Content: `say \"hello\" to Bob`},\n}\n</code></pre></p> <p>Note</p> <p>Look at this sample for a complete sample: examples/17-fake-function-calling</p>"},{"location":"generate-completion/","title":"Generate completion","text":""},{"location":"generate-completion/#generate-completion","title":"Generate completion","text":""},{"location":"generate-completion/#completion","title":"Completion","text":"<p>The simple completion can be used to generate a response for a given prompt with a provided model.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"github.com/parakeet-nest/parakeet/enums/option\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.SetOptions(map[string]interface{}{\n        option.Temperature: 0.5,\n    })\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, question)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"generate-completion/#completion-with-stream","title":"Completion with stream","text":"<pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.Options{\n        Temperature: 0.5,\n    }\n\n    question := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.GenerateStream(ollamaUrl, question,\n        func(answer llm.Answer) error {\n            fmt.Print(answer.Response)\n            return nil\n        })\n\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n}\n</code></pre>"},{"location":"generate-completion/#completion-with-context","title":"Completion with context","text":"<p>see: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion</p> <p>The context can be used to keep a short conversational memory for the next completion.</p> <pre><code>package main\n\nimport (\n    \"github.com/parakeet-nest/parakeet/completion\"\n    \"github.com/parakeet-nest/parakeet/llm\"\n\n    \"fmt\"\n    \"log\"\n)\n\nfunc main() {\n    ollamaUrl := \"http://localhost:11434\"\n    model := \"tinydolphin\"\n\n    options := llm.Options{\n        Temperature: 0.5,\n    }\n\n    firstQuestion := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is James T Kirk?\",\n        Options: options,\n    }\n\n    answer, err := completion.Generate(ollamaUrl, firstQuestion)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n\n    fmt.Println()\n\n    secondQuestion := llm.GenQuery{\n        Model: model,\n        Prompt: \"Who is his best friend?\",\n        Context: answer.Context,\n        Options: options,\n    }\n\n    answer, err = completion.Generate(ollamaUrl, secondQuestion)\n    if err != nil {\n        log.Fatal(\"\ud83d\ude21:\", err)\n    }\n    fmt.Println(answer.Response)\n}\n</code></pre>"},{"location":"openaiapi-support/","title":"OpenAI API support","text":""},{"location":"openaiapi-support/#openai-api-support","title":"OpenAI API support","text":"<p>\u270b Only tested with the <code>gpt-4o-mini</code> model</p> <p>Ollama provides experimental compatibility with parts of the OpenAI API. As it's experimental, I prefer to keep the completion methods of Ollama and OpenAI \"separated\".</p>"},{"location":"openaiapi-support/#chat-completion","title":"Chat completion","text":"<pre><code>openAIUrl := \"https://api.openai.com/v1\"\nmodel := \"gpt-4o-mini\"\n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\nquery := llm.OpenAIQuery{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    //Verbose: true,\n    OpenAIAPIKey: os.Getenv(\"OPENAI_API_KEY\"),\n}\n\nanswer, err := completion.ChatWithOpenAI(openAIUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(answer.Choices[0].Message.Content)\n</code></pre>"},{"location":"openaiapi-support/#chat-completion-with-stream","title":"Chat completion with stream","text":"<pre><code>openAIUrl := \"https://api.openai.com/v1\"\nmodel := \"gpt-4o-mini\"\n\nsystemContent := `You are an expert in Star Trek.`\nuserContent := `Who is Jean-Luc Picard?`\n\nquery := llm.OpenAIQuery{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    //Verbose: true,\n    OpenAIAPIKey: os.Getenv(\"OPENAI_API_KEY\"),\n}\n\ntextResult, err = completion.ChatWithOpenAIStream(openAIUrl, query,\n    func(answer llm.OpenAIAnswer) error {\n        fmt.Print(answer.Choices[0].Delta.Content)\n        return nil\n    })\n\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n</code></pre>"},{"location":"openaiapi-support/#chat-completion-with-tools","title":"Chat completion with tools","text":"<p>\ud83d\udea7 in progress</p>"},{"location":"openaiapi-support/#create-embeddings","title":"Create embeddings","text":"<pre><code>// Create an embedding from the question\nembeddingFromQuestion, err := embeddings.CreateEmbeddingWithOpenAI(\n    openAIUrl,\n    llm.OpenAIQuery4Embedding{\n        Model:        embeddingsModel,\n        Input:       userContent,\n        OpenAIAPIKey: os.Getenv(\"OPENAI_API_KEY\"),\n    },\n    \"unique-id\",\n)\n</code></pre> <p>Note</p> <p>You can find an example in examples/49-embeddings-memory-openai</p>"},{"location":"other-helpers/","title":"Other helpers and Parakeet methods","text":""},{"location":"other-helpers/#get-information-about-a-model","title":"Get Information about a model","text":"<pre><code>llm.ShowModelInformation(url, model string) (llm.ModelInformation, int, error)\n</code></pre> <p><code>ShowModelInformation</code> retrieves information about a model from the specified URL.</p> <p>Parameters:</p> <ul> <li><code>url</code>: the base URL of the API.</li> <li><code>model</code>: the name of the model to retrieve information for.</li> </ul> <p>Returns:</p> <ul> <li><code>ModelInformation</code>: the information about the model.</li> <li><code>int</code>: the HTTP status code of the response.</li> <li><code>error</code>: an error if the request fails.</li> </ul> <p>\u270b Remark: if the model does not exist, it will return an error with a status code of 404.</p> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.ShowModelInformationWithToken(url, model , tokenHeaderName, tokenHeaderValue string) (llm.ModelInformation, int, error)\n</code></pre>"},{"location":"other-helpers/#pull-a-model","title":"Pull a model","text":"<pre><code>llm.PullModel(url, model string) (llm.PullResult, int, error)\n</code></pre> <p><code>PullModel</code> sends a POST request to the specified URL to pull a model with the given name.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to send the request to.</li> <li><code>model</code>: The name of the model to pull.</li> </ul> <p>Returns:</p> <ul> <li><code>PullResult</code>: The result of the pull operation.</li> <li><code>int</code>: The HTTP status code of the response.</li> <li><code>error</code>: An error if the request fails.</li> </ul> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.PullModelWithToken(url, model , tokenHeaderName, tokenHeaderValue string) (llm.PullResult, int, error)\n</code></pre>"},{"location":"other-helpers/#get-the-list-of-the-installed-models","title":"Get the list of the installed models","text":"<pre><code>llm.GetModelsList(url string) (llm.ModelList, int, error)\n</code></pre> <p><code>GetModelsList</code> sends a GET request to the specified URL to fetch the list of the installed models.</p> <p>Parameters:</p> <ul> <li><code>url</code>: The URL to send the request to.</li> </ul> <p>Returns:</p> <ul> <li><code>ModelList</code>: The result of the reques, use the <code>models</code> property to get the list.</li> <li><code>int</code>: The HTTP status code of the response.</li> <li><code>error</code>: An error if the request fails.</li> </ul> <p>If you use a protected Ollama endpoint, use this function:</p> <pre><code>llm.GetModelsList(url , tokenHeaderName, tokenHeaderValue string) (llm.ModelList, int, error)\n</code></pre> <p>Note</p> <p>\ud83d\udc40 you will find a complete example in:</p> <ul> <li>examples/50-llm-information</li> </ul>"},{"location":"parakeet-blog/","title":"Blog Post","text":"<p>Note</p> <p>This page need to be updated with the latest blog posts.</p> <p>Info</p> <p>This is a collection of Parakeet blog posts showcasing the ease of creating GenAI applications with Ollama, Golang, and other tools.</p> <ul> <li>Parakeet, an easy way to create GenAI applications with Ollama and Golang</li> <li>Understand RAG with Parakeet -Function Calling with Ollama, Mistral 7B, Bash and Jq</li> <li>Function Calling with Ollama and LLMs that do not support function calling</li> </ul>"},{"location":"parakeet-demos/","title":"\ud83d\ude80 Parakeet Demos","text":""},{"location":"parakeet-demos/#parakeet-demos","title":"Parakeet Demos","text":"<p>Note</p> <p>These demos need to be updated</p> <p>Info</p> <p>This is a collection of Parakeet demos showcasing the ease of creating GenAI applications with Ollama, Golang, and other tools.</p> <ul> <li>https://github.com/parakeet-nest/parakeet-demo</li> <li>https://github.com/parakeet-nest/tiny-genai-stack</li> </ul>"},{"location":"parakeet-examples/","title":"Parakeet examples &amp; Recipes","text":"<p>Of this repository: https://github.com/parakeet-nest/parakeet/tree/main/examples</p> Item Details 01-generate generate completion 02-generate-stream generate completion with stream 03-chat chat completion 04-chat-stream chat completion with stream 05-context chat with context 06-summary summarize a content 07-explain explain a piece of code 08-embeddings-memory create embeddings in a memory vector store 09-embeddings-bbolt create embeddings in a bbolt vector store 10-chat-conversational-memory chat completion + conversational memory 11-chat-conversational-bbolt persist the conversation of the chat 12-json-output the LLM generates a json output 13-simple-fake-function-calling kind of function calling 14-mistral-function-calling use the tools support of Mistral 15-mistral-function-calling use the tools support of Mistral 16-fake-function-calling make function calling without the tools support 17-fake-function-calling make function calling without the tools support 18-call-functions-for-real call wasm functions with tools 19-mistral-function-calling-tool-support use the new tools support with Mistral 20-play-with-granite generate source code 21-analysis-with-granite analyse source code 22-dockerizing generate a Dockerfile and a Compose file 23-rag-with-chunker rag demo 24-rag-with-splitter rag demo 25-rag-with-regex rag demo 26-docker-cmd teach the docker commands to the LLM 27-generate-token use an authentication token 28-generate-stream-token use an authentication token 29-chat-token use an authentication token 30-chat-stream-token use an authentication token 31-embeddings-memory-token use an authentication token 32-rag-with-redis rag demo with Redis as the vector store 33-rag-with-elastic rag demo with Elasticsearch as the vector store 34-rag-with-bbolt rag demo with Bbolt as the vector store 35-rag-with-bbolt rag demo with Bbolt as the vector store 36-rag-with-asciidoc rag demo with Asciidoc document 37-rag-with-html rag demo with HTML document 38-rag-with-markdown rag demo with Markdown document 39-rag-with-elastic-markdown rag demo with Markdown and Elasticsearch 40-rag-with-elastic-markdown rag demo with Markdown and Elasticsearch 41-prompt-from-repo \ud83d\udea7 42-make-a-slm-smarter experiments to make a SLM an expert on a specific topic 43-function-calling tools demos with SLMs 44-chat-openai chat completion with OpenAI 45-chat-stream-openai chat stream completion with OpenAI 46-create-an-expert \ud83d\udea7 47-function-calling-xp call tools several times with the same prompt 48-testing-models testing various models 49-embeddings-memory-openai create embeddings with OpenAI 50-llm-information get information about LLMs 51-genai-webapp GenAI web application demo"},{"location":"parsing-markdown/","title":"Parsing Markdown","text":"<p>\ud83d\udea7 work in progress</p>"},{"location":"prompt-helpers/","title":"\ud83d\udd28 Prompt Helpers","text":""},{"location":"prompt-helpers/#prompt-helpers","title":"Prompt helpers","text":""},{"location":"prompt-helpers/#meta-prompts","title":"Meta prompts","text":"<p>package: <code>prompt</code></p> <p>Meta-prompts are special instructions embedded within a prompt to guide a language model in generating a specific kind of response.</p> Meta-Prompt Purpose [Brief] What is AI? For a concise answer [In Layman\u2019s Terms] Explain LLM For a simplified explanation [As a Story] Describe the evolution of cars To get the information in story form [Pros and Cons] Is AI useful? For a balanced view with advantages and disadvantages [Step-by-Step] How to do a smart prompt? For a detailed, step-by-step guide [Factual] What is the best pizza of the world? For a straightforward, factual answer [Opinion] What is the best pizza of the world? To get an opinion-based answer [Comparison] Compare pineapple pizza to pepperoni pizza For a comparative analysis [Timeline] What are the key milestones to develop a WebApp? For a chronological account of key events [As a Poem] How to cook a cake? For a poetic description [For Kids] How to cook a cake? For a child-friendly explanation [Advantages Only] What are the benefits of AI? To get a list of only the advantages [As a Recipe] How to cook a cake? To receive the information in the form of a recipe"},{"location":"prompt-helpers/#meta-prompts-methods","title":"Meta prompts methods","text":"<ul> <li><code>prompt.Brief(s string) string</code></li> <li><code>prompt.InLaymansTerms(s string) string</code></li> <li><code>prompt.AsAStory(s string) string</code></li> <li><code>prompt.ProsAndCons(s string) string</code></li> <li><code>prompt.StepByStep(s string) string</code></li> <li><code>prompt.Factual(s string) string</code></li> <li><code>prompt.Opinion(s string) string</code></li> <li><code>prompt.Comparison(s string) string</code></li> <li><code>prompt.Timeline(s string) string</code></li> <li><code>prompt.AsAPoem(s string) string</code></li> <li><code>prompt.ForKids(s string) string</code></li> <li><code>prompt.AdvantagesOnly(s string) string</code></li> <li><code>prompt.AsARecipe(s string) string</code></li> </ul>"},{"location":"protected-endpoint/","title":"Protected endpoint","text":"<p>If your Ollama endpoint is protected with a header token, you can specify the token like this:</p> <pre><code>query := llm.Query{\n    Model: model,\n    Messages: []llm.Message{\n        {Role: \"system\", Content: systemContent},\n        {Role: \"user\", Content: userContent},\n    },\n    Options: options,\n    TokenHeaderName: \"X-TOKEN\",\n    TokenHeaderValue: \"john_doe\",\n}\n</code></pre>"},{"location":"set-options/","title":"How to set the Options of the query","text":"<p>The best way to set the options of the query is to start from the default Ollama options and override the fields you want to change.</p> <pre><code>options := llm.DefaultOptions()\n// override the default value\noptions.Temperature = 0.5\n</code></pre> <p>Or you can use the <code>SetOptions</code> helper. This helper will set the default values for the fields not defined in the map:</p> <p>Define only the fields you want to override: <pre><code>options := llm.SetOptions(map[string]interface{}{\n  \"Temperature\": 0.5,\n})\n</code></pre></p> <p>Or use the <code>SetOptions</code> helper with the <code>option</code> enums: <pre><code>options := llm.SetOptions(map[string]interface{}{\n  option.Temperature: 0.5,\n  option.RepeatLastN: 2,\n})\n</code></pre></p> <p>Note</p> <p>Before, the JSON serialization of the <code>Options</code> used the <code>omitempty</code> tag.</p> <p>The <code>omitempty</code> tag prevents a field from being serialised if its value is the zero value for the field's type (e.g., 0.0 for float64).</p> <p>That means when <code>Temperature</code> equals <code>0.0</code>, the field is not serialised (then Ollama will use the <code>Temperature</code> default value, which equals <code>0.8</code>).</p> <p>The problem will happen for every value equal to <code>0</code> or <code>0.0</code></p> <p>Since now, the <code>omitempty</code> tag is removed from the <code>Options</code> struct.</p>"},{"location":"tools/","title":"Function Calling with tool support","text":"<p>Ollama API: chat request with tools https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-with-tools</p> <p>Since Ollama <code>0.3.0</code>, Ollama supports tools calling, blog post: https://ollama.com/blog/tool-support. A list of supported models can be found under the Tools category on the models page: https://ollama.com/search?c=tools</p>"},{"location":"tools/#define-a-list-of-tools","title":"Define a list of tools","text":"<p>use a supported model</p> <pre><code>model := \"mistral:7b\"\n\ntoolsList := []llm.Tool{\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"hello\",\n            Description: \"Say hello to a given person with his name\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"name\": {\n                        Type:        \"string\",\n                        Description: \"The name of the person\",\n                    },\n                },\n                Required: []string{\"name\"},\n            },\n        },\n    },\n    {\n        Type: \"function\",\n        Function: llm.Function{\n            Name:        \"addNumbers\",\n            Description: \"Make an addition of the two given numbers\",\n            Parameters: llm.Parameters{\n                Type: \"object\",\n                Properties: map[string]llm.Property{\n                    \"a\": {\n                        Type:        \"number\",\n                        Description: \"first operand\",\n                    },\n                    \"b\": {\n                        Type:        \"number\",\n                        Description: \"second operand\",\n                    },\n                },\n                Required: []string{\"a\", \"b\"},\n            },\n        },\n    },\n}\n</code></pre>"},{"location":"tools/#set-the-tools-property-of-the-query","title":"Set the Tools property of the query","text":"<ul> <li>set the <code>Temperature</code> to <code>0.0</code></li> <li>you don't need to set the row mode to true</li> <li>set <code>query.Tools</code> with <code>toolsList</code></li> </ul> <pre><code>messages := []llm.Message{\n    {Role: \"user\", Content: `say \"hello\" to Bob`},\n}\n\noptions := llm.SetOptions(map[string]interface{}{\n    option.Temperature: 0.0,\n    option.RepeatLastN: 2,\n    option.RepeatPenalty: 2.0,\n})\n\nquery := llm.Query{\n    Model:    model,\n    Messages: messages,\n    Tools:    toolsList,\n    Options:  options,\n    Format:   \"json\",\n}\n</code></pre>"},{"location":"tools/#run-the-completion","title":"Run the completion","text":"<pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\n// It's a []map[string]interface{}\ntoolCalls := answer.Message.ToolCalls\n\n// Convert toolCalls into a JSON string\njsonBytes, err := json.Marshal(toolCalls)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n// Convert JSON bytes to string\nresult := string(jsonBytes)\n\nfmt.Println(result)\n</code></pre> <p>The result will look like this: <pre><code>[{\"function\":{\"arguments\":{\"name\":\"Bob\"},\"name\":\"hello\"}}]\n</code></pre></p>"},{"location":"tools/#or-you-can-use-the-toolcallstojsonstring-helper","title":"Or you can use the <code>ToolCallsToJSONString</code> helper","text":"<p><pre><code>answer, err = completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\nresult, err = answer.Message.ToolCallsToJSONString()\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre> The result will look like this: <pre><code>[{\"function\":{\"arguments\":{\"name\":\"Bob\"},\"name\":\"hello\"}}]\n</code></pre></p> <p>Note</p> <p>Look here for a complete sample: examples/19-mistral-function-calling-tool-support</p>"},{"location":"tools/#or-better-you-can-use-the-toolcallstojsonstring-helper","title":"Or (better) you can use the <code>ToolCallsToJSONString</code> helper","text":"<pre><code>answer, err := completion.Chat(ollamaUrl, query)\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\n\nresult, err := answer.Message.ToolCalls[0].Function.ToJSONString()\nif err != nil {\n    log.Fatal(\"\ud83d\ude21:\", err)\n}\nfmt.Println(result)\n</code></pre> <p>The result will look like this: <pre><code>{\"name\":\"hello\",\"arguments\":{\"name\":\"Bob\"}}\n</code></pre></p> <p>Note</p> <p>Look at these samples:</p> <ul> <li>examples/43-function-calling/01-xlam</li> <li>examples/43-function-calling/02-qwen2tools</li> </ul>"},{"location":"verbose-mode/","title":"Verbose mode","text":"<p>You can activate the \"verbose mode\" with all kinds of completions.</p> <p><pre><code>options := llm.SetOptions(map[string]interface{}{\n  option.Temperature: 0.5,\n  option.RepeatLastN: 2,\n  option.RepeatPenalty: 2.0,\n  option.Verbose: true,\n})\n</code></pre> You will get an output like this (with the query and the completion):</p> <pre><code>[llm/query] {\n  \"model\": \"deepseek-coder\",\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You are an expert in computer programming.\\n\\tPlease make friendly answer for the noobs.\\n\\tAdd source code examples if you can.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"I need a clear explanation regarding the following question:\\n\\tCan you create a \\\"hello world\\\" program in Golang?\\n\\tAnd, please, be structured with bullet points\"\n    }\n  ],\n  \"options\": {\n    \"repeat_last_n\": 2,\n    \"temperature\": 0.5,\n    \"repeat_penalty\": 2,\n    \"Verbose\": true\n  },\n  \"stream\": false,\n  \"prompt\": \"\",\n  \"context\": null,\n  \"tools\": null,\n  \"TokenHeaderName\": \"\",\n  \"TokenHeaderValue\": \"\"\n}\n\n[llm/completion] {\n  \"model\": \"deepseek-coder\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sure, here's a simple \\\"Hello, World!\\\" program in Golang.\\n\\t1. First, you need to have Golang installed on your machine.\\n\\t2. Open your text editor, and write the following code:\\n\\t```go\\n\\tpackage main\\n\\timport \\\"fmt\\\"\\n\\tfunc main() {\\n\\t    fmt.Println(\\\"Hello, World!\\\")\\n\\t} \\n\\t```\\n\\t3. Save the file with a `.go` extension (like `hello.go`).\\n\\t4. In your terminal, navigate to the directory containing the `.go` file.\\n\\t5. Run the program with the command:\\n\\t```\\n\\tgo run hello.go\\n\\t```\\n\\t6. If everything goes well, you should see \\\"Hello, World!\\\" printed in your terminal.\\n\\t7. If there's an error, you will see the error message.\\n\\t8. If everything is correct, you'll see \\\"Hello, World!\\\" printed in your terminal.\\n\"\n  },\n  \"done\": true,\n  \"response\": \"\",\n  \"context\": null,\n  \"created_at\": \"2024-08-19T05:57:23.979361Z\",\n  \"total_duration\": 3361191958,\n  \"load_duration\": 2044932125,\n  \"prompt_eval_count\": 79,\n  \"prompt_eval_duration\": 95034000,\n  \"eval_count\": 222,\n  \"eval_duration\": 1216689000\n}\n</code></pre>"},{"location":"wasm-plugins/","title":"WASM Plugins","text":""},{"location":"wasm-plugins/#wasm-plugins","title":"Wasm plugins","text":"<p>The release <code>0.0.6</code> of Parakeet brings the support of WebAssembly thanks to the Extism project. That means you can write your own wasm plugins for Parakeet to add new features (for example, a chunking helper for doing RAG) with various languages (Rust, Go, C, ...).</p> <p>Or you can use the Wasm plugins with the \"Function Calling\" feature, which is implemented in Parakeet.</p> <p>Note</p> <p>You can find an example of \"Wasm Function Calling\" in examples/18-call-functions-for-real - the wasm plugin is located in the <code>wasm</code> folder and it is built with TinyGo.</p> <p>\ud83d\udea7 more samples to come.</p>"}]}