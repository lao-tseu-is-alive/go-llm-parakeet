# Make a Small Language Model Smarter: Teach Docker Commands

Let's see how we can lend a hand to our LLM so it can help us generate relevant responses.

## Add some context to the prompt

An LLM is just a text generator that needs precise information to generate relevant responses (basically, you also need to provide the answers in the prompt).

The prompt is composed of instructions for the LLM and the user's questions.
But you can also add answers (context) in the prompt to help the LLM generate more relevant responses.

I will update the code to add some context to the prompt:

1. I will add a system message to provide more information about the task.
2. I will add some examples of questions and Docker commands to the prompt.
3. I will update the query to include the context in the prompt.

```golang
func main() {
	ollamaUrl := "http://localhost:11434"

	smallChatModel := "qwen2:0.5b"

	// 1Ô∏è‚É£ Make the system content more informative
	systemContent := `instruction: 
	translate the user question in docker command using the given context.
	Stay brief.`

	// 2Ô∏è‚É£ Prepare the context content with some examples
	contextContent := `<context>
		<doc>
		input: Give me a list of all containers, indicating their status as well.
		output: docker ps -a
		</doc>
		<doc>
		input: List all containers with Ubuntu as their ancestor.
		output: docker ps --filter 'ancestor=ubuntu'
		</doc>
		<doc>
		input: Give me a list of all the local Docker images.
		output: docker images
		</doc>
	</context>
	`

	options := llm.SetOptions(map[string]interface{}{
		option.Temperature: 0.0,
		option.RepeatLastN: 2,
		option.RepeatPenalty: 3.0,
		option.TopK: 10,
		option.TopP: 0.5,
	})

	for {
		question := input(smallChatModel)
		if question == "bye" {
			break
		}

		// Prepare the query
		query := llm.Query{
			Model: smallChatModel,
			Messages: []llm.Message{
				{Role: "system", Content: systemContent},
				// 3Ô∏è‚É£ Add the context to the prompt
				{Role: "system", Content: contextContent},
				{Role: "user", Content: question},
			},
			Options: options,
		}

		// Answer the question
		_, err := completion.ChatStream(ollamaUrl, query,
			func(answer llm.Answer) error {
				fmt.Print(answer.Message.Content)
				return nil
			})

		if err != nil {
			log.Fatal("üò°:", err)
		}
		fmt.Println()
	}
}
```
> **Note**: I added some tags (`<context><doc></doc></context>`) to the context content to make it more readable/understandable for the LLM. I found that the LLm's answers were more accurate when I delimited the examples in context. This seems to help the LLM focus on the most appropriate example.


Ok, now, let's run the program again and see if Qwen2:0.5b LLM is smarter.

```bash
go run main.go
```

Then, ask (again) the following questions:

- "Give me a list of all the local Docker images."
- "Give me a list of all containers, indicating their status as well."
- "List all containers with Ubuntu as their ancestor."

You should see more accurate Docker commands generated by the LLM.