# Make a Small Language Model Smarter: Teach Docker Commands

## Use Embeddings to enhance the LLM

We will update again the source code to use the embeddings we created in the previous step.

This time, the application interacts with two LLM (**all-minilm:33m** and **qwen2:0.5b**) to translate user questions into Docker commands **using a given context**: 

```golang
func main() {
	// 1Ô∏è‚É£ Configuration
	ollamaUrl := "http://localhost:11434"
	smallChatModel := "qwen2:0.5b"
	embeddingsModel := "all-minilm:33m"

	// 2Ô∏è‚É£ System Content
	systemContent := `instruction: 
	translate the user question in docker command using the given context.
	Stay brief.`

	// 3Ô∏è‚É£ Embedding Store
	store := embeddings.BboltVectorStore{}
	store.Initialize("../embeddings.db")

	options := llm.SetOptions(map[string]interface{}{
		option.Temperature: 0.0,
		option.RepeatLastN: 2,
		option.RepeatPenalty: 3.0,
		option.TopK: 10,
		option.TopP: 0.5,
	})

	// 4Ô∏è‚É£ Main Loop
	for {
		question := input(smallChatModel)
		if question == "bye" {
			break
		}

		// 5Ô∏è‚É£ Create an embedding from the question
		embeddingFromQuestion, err := embeddings.CreateEmbedding(
			ollamaUrl,
			llm.Query4Embedding{
				Model:  embeddingsModel,
				Prompt: question,
			},
			"question",
		)
		if err != nil {
			log.Fatalln("üò°:", err)
		}

		// 6Ô∏è‚É£ Search for similarity
		fmt.Println("üîé searching for similarity...")
		similarities, _ := store.SearchTopNSimilarities(embeddingFromQuestion, 0.4, 3)

		// 7Ô∏è‚É£ Generate context content
		contextContent := embeddings.GenerateContextFromSimilarities(similarities)
		fmt.Println("üéâ similarities:", len(similarities))

		// 8Ô∏è‚É£ Prepare the query
		query := llm.Query{
			Model: smallChatModel,
			Messages: []llm.Message{
				{Role: "system", Content: systemContent},
				{Role: "system", Content: contextContent},
				{Role: "user", Content: question},
			},
			Options: options,
		}

		// 9Ô∏è‚É£ Answer the question
		_, err = completion.ChatStream(ollamaUrl, query,
			func(answer llm.Answer) error {
				fmt.Print(answer.Message.Content)
				return nil
			})

		if err != nil {
			log.Fatal("üò°:", err)
		}

		fmt.Println()
	}
}
```

1. **Configuration**: Sets up URLs and model names for the Chqt LLM and embeddings LLM.
2. **System Content**: Defines the instruction for the language model to translate user questions into Docker commands.
3. **Embedding Store**: Initializes a vector store for embeddings using a BoltDB database.
4. **Main Loop**: Continuously prompts the user for input until the user types "bye".
5. **Embedding Creation**: Creates an embedding for the user's question using the specified embeddings model (**all-minilm:33m**).
6. **Similarity Search**: Searches for similar embeddings in the vector store.
7. **Context Generation**: Generates context content from the found similarities.
8. **Query Preparation**: Prepares a query for the Chat LLM (**qwen2:0.5b**) with the system content, context content, and user question.
9. **Answer Handling**: Sends the query to **qwen2:0.5b** and prints the response.


**Remark 1**: This line of code `similarities, _ := store.SearchTopNSimilarities(embeddingFromQuestion, 0.4, 3)` searches the vector store for the top `3` embeddings that are most similar to `embeddingFromQuestion` with a similarity score above `0.4`. The results are stored in the `similarities` variable. 

**Remark 2**: `SearchTopNSimilarities` uses the [cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity) calculation.

**Remark 3**: Finding a not-too-high number of similarities allows not to overload the context with unnecessary information and thus helps the SLM to maintain focus.

Ok, now, let's run the program again and see if Qwen2:0.5b LLM is smarter on other commands.

```bash
go run main.go
```

Then, ask (again) the following questions:

- "Give me a list of all the local Docker images."
- "Give me a list of all containers, indicating their status as well."
- "List all containers with Ubuntu as their ancestor."
- "Can you list down the images that are dangling?"
- "Show the list of nginx images in the store."
- "Show the list of redis images in the store."

You should see more accurate Docker commands generated by the LLM.


The dataset I found on HuggingFace isn't perfect (it's missing quite a few Docker commands, like `docker build`, or Docker Compose commands, etc...), but it gives you the approach to "augment" an LLM without having to do fine-tuning.

So you can see that with a dataset on specific information and appropriate prompts, it is entirely possible to use an SLM in a useful and relevant way for your applications.