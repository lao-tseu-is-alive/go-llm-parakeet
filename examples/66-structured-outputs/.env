# if working from a container
#OLLAMA_HOST=http://host.docker.internal:11434
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_LLM_CHAT=qwen2.5:1.5b

MODEL_RUNNER_BASE_URL=http://localhost:12434
MODEL_RUNNER_LLM_CHAT=ai/qwen2.5:latest

OPENAI_API_KEY=<YOUR_OPENAI_API_KEY>
OPENAI_BASE_URL=https://api.openai.com
OPENAI_LLM_CHAT= gpt-4o-mini